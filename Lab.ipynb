{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specs of cifar10:\n",
      "X_train shape: (50000, 32, 32, 3)\n",
      "X_test shape: (10000, 32, 32, 3)\n",
      "y_train shape: (50000, 1)\n",
      "y_test shape: (10000, 1)\n",
      "Flatten y_train shape: (50000,)\n",
      "Flatten y_test shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "(X_train,y_train),(X_test,y_test)=cifar10.load_data()\n",
    "print(\"Specs of cifar10:\\nX_train shape:\",X_train.shape)\n",
    "print(\"X_test shape:\",X_test.shape)\n",
    "print(\"y_train shape:\",y_train.shape)\n",
    "print(\"y_test shape:\",y_test.shape)\n",
    "#flatten y train data & y test data\n",
    "y_train=y_train.reshape(-1)\n",
    "y_test=y_test.reshape(-1)\n",
    "print(\"Flatten y_train shape:\",y_train.shape)\n",
    "print(\"Flatten y_test shape:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load feature_extraction.py\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "# TODO: import Keras layers you need here\n",
    "from keras.layers import Input,Flatten,Dense\n",
    "from keras.models import Model\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# command line flags\n",
    "flags.DEFINE_string('training_file', '', \"Bottleneck features training file (.p)\")\n",
    "flags.DEFINE_string('validation_file', '', \"Bottleneck features validation file (.p)\")\n",
    "flags.DEFINE_integer('epochs', 50, \"The number of epochs.\")\n",
    "flags.DEFINE_integer('batch_size', 256, \"The batch size.\")\n",
    "\n",
    "def load_bottleneck_data(training_file, validation_file):\n",
    "    \"\"\"\n",
    "    Utility function to load bottleneck features.\n",
    "\n",
    "    Arguments:\n",
    "        training_file - String\n",
    "        validation_file - String\n",
    "    \"\"\"\n",
    "    print(\"Training file\", training_file)\n",
    "    print(\"Validation file\", validation_file)\n",
    "\n",
    "    with open(training_file, 'rb') as f:\n",
    "        train_data = pickle.load(f)\n",
    "    with open(validation_file, 'rb') as f:\n",
    "        validation_data = pickle.load(f)\n",
    "\n",
    "    X_train = train_data['features']\n",
    "    y_train = train_data['labels']\n",
    "    X_val = validation_data['features']\n",
    "    y_val = validation_data['labels']\n",
    "\n",
    "    return X_train, y_train, X_val, y_val\n",
    "\n",
    "\n",
    "def main(_):\n",
    "    # load bottleneck data\n",
    "    X_train, y_train, X_val, y_val = load_bottleneck_data(FLAGS.training_file, FLAGS.validation_file)\n",
    "\n",
    "    print(X_train.shape, y_train.shape)\n",
    "    print(X_val.shape, y_val.shape)\n",
    "\n",
    "    # TODO: define your model and hyperparams here\n",
    "    # make sure to adjust the number of classes based on\n",
    "    # the dataset\n",
    "    # 10 for cifar10\n",
    "    # 43 for traffic\n",
    "    nb_classes = len(np.unique(y_train))\n",
    "    input_shape=X_train.shape[1:]\n",
    "    inp=Input(shape=input_shape)\n",
    "    x=Flatten()(inp)\n",
    "    x=Dense(nb_classes,activation='softmax')(x)\n",
    "    model=Model(inp,x)\n",
    "    model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "    # TODO: train your model here\n",
    "    model.fit(X_train,y_train,nb_epoch=FLAGS.epochs,batch_size=FLAGS.batch_size,validation_data=(X_val,y_val),shuffle=True)\n",
    "    \n",
    "\n",
    "# parses flags and calls the `main` function above\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider feature extraction when ...\n",
    "\n",
    "... the new dataset is small and similar to the original dataset. The higher-level features learned from the original dataset should transfer well to the new dataset.\n",
    "\n",
    "Consider finetuning when ...\n",
    "\n",
    "... the new dataset is large and similar to the original dataset. Altering the original weights should be safe because the network is unlikely to overfit the new, large dataset.\n",
    "\n",
    "... the new dataset is small and very different from the original dataset. You could also make the case for training from scratch. If you choose to finetune, it might be a good idea to only use features from the first few layers of the pre-trained network; features from the final layers of the pre-trained network might be too specific to the original dataset.\n",
    "\n",
    "Consider training from scratch when ...\n",
    "\n",
    "... the dataset is large and very different from the original dataset. In this case we have enough data to confidently train from scratch. However, even in this case it might be beneficial to initialize the entire network with pretrained weights and finetune it on the new dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
